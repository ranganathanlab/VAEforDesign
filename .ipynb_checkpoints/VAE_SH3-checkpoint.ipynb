{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein design VAE\n",
    "### Xinran Lian, Andrew Ferguson, Rama Ranganathan\n",
    "---\n",
    "All codes should be run under Python3.  \n",
    "Prior to running this material, please \n",
    "* Make sure you already installed the newest version of the following python packages: **numpy, pandas, numba, scipy, matplotlib, torch, sklearn, Bio**. [Conda environment](https://www.anaconda.com) will be helpful.   \n",
    "\n",
    "* If your input sequences are not aligned, you are recommended to align it with pySCA/scaProcessMSA. See SCA documention [here](https://ranganathanlab.gitlab.io/pySCA/)  \n",
    "  We have a shell script *runsca_SH3* for MSA of SH3, it is necessary if you want to evaluate the VAE model with SCA (statistical coupling analysis). Please install all SCA dependencys before excuting this script from the command line as:  \n",
    "  \n",
    "  ./runsca_SH3.sh\n",
    "\n",
    "\n",
    "* Execute the scripts from the command line as follows:   \n",
    "\n",
    "  cd source  \n",
    "  ./preprocessing.py ../Inputs/sh3_59.fasta -n SH3  \n",
    "  ./train_model.py -n SH3  \n",
    "  \n",
    "  \n",
    "* Additionally, please run the following script to generate new sequences:  \n",
    "\n",
    "  ./Generate_many_seqs.py -g 1000 -n SH3\n",
    "---\n",
    "Explain roles of each script:  \n",
    "* *preprocessing.py*:  \n",
    "  Convert the fasta file to ont-hot Potts representation.  \n",
    "   \n",
    "   \n",
    "* *train_model.py*:  \n",
    "  Train the VAE model  \n",
    "  \n",
    "  \n",
    "* *Generate_many_seqs_SH3.py*: \n",
    "  Generate new sequences and compute their  \n",
    "    * $log(tr(H^TP))$\n",
    "    * Min Hamming distance from MSA\n",
    "    (Please run ./Generate_many_seqs_SH3.py --help  \n",
    "    for details of the arguments.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division \n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "from numba import jit\n",
    "import time\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import scoreatpercentile \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "sys.path.append('./source')\n",
    "import toolkit\n",
    "import scaTools as sca\n",
    "sys.path.append('../')\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Import the dataset and VAE model\n",
    "\n",
    "### Import dataset\n",
    "We are going to use the following annotations in this notebook:\n",
    "* **$N$** number of samples\n",
    "* **$q$** number of one-hot features\n",
    "* **$n$** number of amino acid residues in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The -n argument of ./preprocessing.py\n",
    "proteinname = 'SH3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Outputs/'\n",
    "\n",
    "# Import the Potts sequence. \n",
    "parameters = pickle.load(open(path + proteinname + \".db\", 'rb'))\n",
    "\n",
    "q_n = parameters['q_n'] # Number of possible residues on each position\n",
    "v_traj_onehot = parameters['onehot'] # The one-hot encoded sequences\n",
    "\n",
    "N=np.size(v_traj_onehot,axis=0) #number of samples \n",
    "q=np.size(v_traj_onehot,axis=1) #number of one-hot features\n",
    "n=np.size(q_n) # number of amino acid residues in a sequences\n",
    "\n",
    "print(\"Number of samples = \", N)\n",
    "print(\"Number of features = \", q)\n",
    "print('Number of amino acids = ', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, q, d):\n",
    "        super(VAE, self).__init__()\n",
    "        self.hsize=int(1.5*q) # size of hidden layer\n",
    "        \n",
    "        self.en1 = nn.Linear(q, self.hsize)\n",
    "        self.en2 = nn.Linear(self.hsize, self.hsize) #\n",
    "        self.en3 = nn.Linear(self.hsize, self.hsize)\n",
    "        self.en_mu = nn.Linear(self.hsize, d)\n",
    "        self.en_std = nn.Linear(self.hsize, d) # Is it logvar?\n",
    "        \n",
    "        self.de1 = nn.Linear(d, self.hsize)\n",
    "        self.de2 = nn.Linear(self.hsize, self.hsize) #\n",
    "        self.de22 = nn.Linear(self.hsize, self.hsize)\n",
    "        self.de3 = nn.Linear(self.hsize, q)     \n",
    " \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(self.hsize) # batchnorm layer\n",
    "        self.bn2 = nn.BatchNorm1d(self.hsize)\n",
    "        self.bn3 = nn.BatchNorm1d(self.hsize)\n",
    "        self.bnfinal = nn.BatchNorm1d(q)  \n",
    "\n",
    "        #replace tanh with relu\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode a batch of samples, and return posterior parameters for each point.\"\"\"\n",
    "        x = self.tanh(self.en1(x)) # first encode\n",
    "        x = self.dropout1(x) \n",
    "        x = self.tanh(self.en2(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.tanh(self.en3(x)) # second encode\n",
    "        return self.en_mu(x), self.en_std(x) # third (final) encode, return mean and variance\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode a batch of latent variables\"\"\"\n",
    "        z = self.tanh(self.de1(z))\n",
    "        z = self.bn2(z)\n",
    "        z = self.tanh(self.de2(z))\n",
    "        z = self.dropout2(z)\n",
    "        z = self.tanh(self.de22(z))\n",
    "        \n",
    "        # residue-based softmax\n",
    "        # - activations for each residue in each position ARE constrained 0-1 and ARE normalized (i.e., sum_q p_q = 1)\n",
    "        z = self.bn3(z)\n",
    "        z = self.de3(z)\n",
    "        z = self.bnfinal(z)\n",
    "        z_normed = torch.FloatTensor() # empty tensor?\n",
    "        z_normed = z_normed.to(device) # store this tensor in GPU/CPU\n",
    "        for j in range(n):\n",
    "            start = np.sum(q_n[:j])\n",
    "            end = np.sum(q_n[:j+1])\n",
    "            z_normed_j = self.softmax(z[:,start:end])\n",
    "            z_normed = torch.cat((z_normed,z_normed_j),1)\n",
    "        return z_normed\n",
    "    \n",
    "    def reparam(self, mu, logvar): \n",
    "        \"\"\"Reparameterisation trick to sample z values. \n",
    "        This is stochastic during training, and returns the mode during evaluation.\n",
    "        Reparameterisation solves the problem of random sampling is not continuous, which is necessary for gradient descent\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_() \n",
    "            eps = std.data.new(std.size()).normal_() # normal distribution\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu      \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Takes a batch of samples, encodes them, and then decodes them again to compare.\"\"\"\n",
    "        mu, logvar = self.encode(x.view(-1, q)) # get mean and variance\n",
    "        z = self.reparam(mu, logvar) # sampling latent variable z from mu and logvar\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    def loss(self, reconstruction, x, mu, logvar): \n",
    "        \"\"\"ELBO assuming entries of x are binary variables, with closed form KLD.\"\"\"\n",
    "        bce = torch.nn.functional.binary_cross_entropy(reconstruction, x.view(-1, q))\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        # Normalise by same number of elements as in reconstruction\n",
    "        KLD /= x.view(-1, q).data.shape[0] * q \n",
    "        return bce + KLD\n",
    "    \n",
    "    def get_z(self, x):\n",
    "        \"\"\"Encode a batch of data points, x, into their z representations.\"\"\"\n",
    "        mu, logvar = self.encode(x.view(-1, q))\n",
    "        return self.reparam(mu, logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained VAE\n",
    "d=3\n",
    "model = VAE(q,d)\n",
    "model_init = VAE(q,d)\n",
    "model.load_state_dict(torch.load('source/VAE.pyt',map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Validation\n",
    "\n",
    "### Internal validation: how well the model reconstruct natural sequences\n",
    "**1. Reconstruction accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct ont-hot representation for input sequences by the trained VAE by encoding and decoding.\n",
    "def reconstruct(model, sequence_list, q_n):\n",
    "    model.eval()\n",
    "    real_ref = torch.FloatTensor(sequence_list) \n",
    "    pred_ref, mu_ref, logvar_ref = model(real_ref)\n",
    "    pred_ref = pred_ref.cpu().detach().numpy().reshape([-1,sum(q_n)])\n",
    "    \n",
    "    sequence_list = sequence_list.reshape([-1,sum(q_n)])\n",
    "    length = np.size(sequence_list,axis=0)\n",
    "    reconstruct_nothot = np.zeros([length,len(q_n)])\n",
    "    \n",
    "    for i in range(length):\n",
    "        for j in range(len(q_n)):\n",
    "            start = np.sum(q_n[:j])\n",
    "            end = np.sum(q_n[:j+1])\n",
    "            reconstruct_nothot[i,j] = np.argmax(pred_ref[i,start:end])\n",
    "    return (reconstruct_nothot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_nohot_list = toolkit.convert_nohot(v_traj_onehot, q_n)\n",
    "z_test = model.get_z(torch.FloatTensor(v_traj_onehot)).cpu().detach().numpy() \n",
    "test_recons = reconstruct(model, v_traj_onehot, q_n)\n",
    "Hamming_list = [int(n*distance.hamming(test_recons[i],real_nohot_list[i])) for i in range(len(v_traj_onehot))]\n",
    "\n",
    "mean_acc = np.mean((test_recons==real_nohot_list).astype(int),axis=0) # List of mean reconstruction accuracy\n",
    "\n",
    "print(\"Mean accuracy = %2f\" %np.mean(mean_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the variables:  \n",
    "  \n",
    "* **real_nohot_list** int representation of MSA  \n",
    "* **z_test** The latent variables  \n",
    "* **test recons** int representation of reconstructed sequences  \n",
    "* **Hamming_list** Hamming distances from the original MSA and reconstructed sequences.   \n",
    "* **mean_acc** List of mean reconstruction accuracy at each position ($accuracy^i$), namely  \n",
    "  \n",
    "$accuracy^i=\\frac{1}{N}\\sum_{i=1}^{N} \\delta(a_i- \\hat{a}_i)$ \n",
    "  \n",
    "Then we plot the histogram of reconstruction Hamming distances for MSA and bar plot of $accuracy^i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.hist(Hamming_list, max(Hamming_list))\n",
    "plt.xlabel('Hamming distance (max = %d)' %n)\n",
    "plt.ylabel('number of sequences\\n (N ='+str(len(v_traj_onehot))+')')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,3))\n",
    "plt.bar(np.arange(n),mean_acc)\n",
    "plt.xticks(np.arange(0,n,5))\n",
    "plt.xlim([-1,n])\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('$accuracy^i$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare reconstruction accuracy with first order statistics, at beginning we load the SCA database of MSA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db = pickle.load(open('Inputs/sh3_59.db','rb'))\n",
    "    Dseq = db['sequence']  #the results of scaProcessMSA\n",
    "    Dsca = db['sca']       #the results of scaCore\n",
    "    Dsect = db['sector']   #the results of scaSectorID\n",
    "\n",
    "    Di_list = Dsca['Di']\n",
    "except FileNotFoundError:\n",
    "    print('Please firstly proceed SCA pipeline on your fasta file. (See begining of this notebook)')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_tmp = (test_recons==real_nohot_list).astype(int)\n",
    "mean_acc = np.mean(acc_tmp,axis=0) # List of mean reconstruction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the $accuracy^i$ with Kullback-Leibler relative entropy $D_i$ at each position $i$:   \n",
    "  \n",
    "  $D_i^a = f_i^aln\\frac{f_i^a}{p^a}+(1-f^a_i)ln\\frac{1-f_i^a}{a-p^a}$  \n",
    "  \n",
    "  $D_i = \\sum_{a=0}^{20}f^a_iln(\\frac{f_i^a}{p^a})$ \n",
    "    \n",
    "  where  \n",
    "*  $a_i$ is the amino acid at position $i$ in the real sequence.   \n",
    "*  $p^a$ is the background distribution of amino acid $a$  \n",
    "*  $f_i^a$ is frequency of amino acid $a$ at position $i$\n",
    "  \n",
    "The following scatter diagram is going to show the relationship between $D_i$ and $accuracy^i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size = 15\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(5,5))\n",
    "ax1.set_ylabel('$accuracy_i$', fontsize=set_size)\n",
    "ax1.set_xlabel('$D_i$', fontsize=set_size)\n",
    "ax1.scatter(Di_list, mean_acc[:-1], c='k',s = set_size)\n",
    "ax1.tick_params(axis='y',labelsize=set_size)\n",
    "ax1.tick_params(axis='x',labelsize=set_size)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. VAE log probability**\n",
    "  \n",
    "Using the model trained by MSA, the probability of each input sequence can be estimated as \n",
    "  \n",
    "$logP(x|z) \\propto log[tr(H^TP)]$  \n",
    "\n",
    "where  \n",
    "* $H$ is an $21Ã—L$ matrix representing the one-hot encoding of a sequence  \n",
    "* $P$ is the probability weight matrix generated by feeding the network a sequence. \n",
    "\n",
    "For the first step, we compute **log_p_list**, which is the list of $log[tr(H^TP)]$ for each sequence. \n",
    "\n",
    "*Ref. https://arxiv.org/abs/1712.03346*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_time = time.time()\n",
    "\n",
    "pred_ref,_,_ = model(torch.FloatTensor(v_traj_onehot))\n",
    "p_weight = pred_ref.cpu().detach().numpy()\n",
    "log_p_list = np.array(toolkit.make_logP(v_traj_onehot, p_weight,q_n))\n",
    "    \n",
    "end_time = time.time()\n",
    "print(\"Elapsed time %.2f (s)\" % (end_time - st_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we show the $H$ and $P$ matrix of a sequence of interest.  \n",
    "  \n",
    "**Hint**: Run this block, text the index of your interested sequence in the text box on the right side of the scroll bar, then press Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_matrices(test_seq):\n",
    "    print(\"Reconstruction Hamming distance: \", Hamming_list[test_seq])\n",
    "    cmap = 'hot'\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(15, 5))\n",
    "    gened = axes[0].imshow(toolkit.make_matrix(v_traj_onehot[test_seq], n, q_n),cmap = cmap)\n",
    "    axes[0].set_title('one-hot encoding matirx $H$')\n",
    "\n",
    "    divider = make_axes_locatable(axes[0])\n",
    "    cax = divider.append_axes(\"right\", size=\"1%\", pad=0.05)\n",
    "    plt.colorbar(gened, cax=cax)\n",
    "\n",
    "    msa = axes[1].imshow(toolkit.make_matrix(p_weight[test_seq], n, q_n),cmap = cmap)\n",
    "    axes[1].set_title('Reconstruction probability weight matrix $P$')\n",
    "\n",
    "    divider = make_axes_locatable(axes[1])\n",
    "    cax = divider.append_axes(\"right\", size=\"1%\", pad=0.05)\n",
    "    plt.colorbar(gened, cax=cax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "interact(show_matrices, test_seq = N-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot the adjusted log probability, which is $log[tr(H^TP)]$ normalized by setting completely accurate reconstruction as zero, equally \n",
    "\n",
    "$log[tr(H^{T}P)] - log(L)$  v.s. reconstruction Hamming distance.  \n",
    "  \n",
    "The log Probability is negatively correlated with reconstruction Hamming distance. In other words, higher reconstruction accuracy means higher probability of a sequence to occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(Hamming_list,log_p_list, s = 5)\n",
    "plt.xlabel('Reconstruction Hamming distance')\n",
    "plt.ylabel('$log(tr(H^{T}P)) - log(L)$')\n",
    "#plt.ylim([-np.log(n),0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [7,5])\n",
    "print('maxlogP = %.3f' %max(log_p_list))\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.hist(log_p_list, 30)\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('$logP$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Mapping MSA in the 3D VAE latent space**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE = np.load('Input/RE.npz')['RE_mapping']\n",
    "records_MSA = parameters['seq']\n",
    "for num, i in enumerate(records_MSA):\n",
    "    if 'DADDA' in i:\n",
    "        sho1 = i\n",
    "        sho1_id = num\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "fig, ax = plt.subplots(figsize = [5,4])\n",
    "N_hist1, bins, patches = ax.hist(RE,35,color = 'r', edgecolor='k',linewidth=1)\n",
    "ax.set_xlabel('Relative enrichment (Sho1 = 0)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('in YPD + 1M KCl')\n",
    "for i in range(0,18):    \n",
    "    patches[i].set_facecolor('b')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=[21,8])\n",
    "c = RE\n",
    "cmap = plt.cm.coolwarm\n",
    "s1 = axs[0].scatter(z_test[:,0], z_test[:,1],s=15, c = c, cmap=cmap)#, vmax = -0.5730, vmin = -2.63)\n",
    "s2 = axs[1].scatter(z_test[:,0], z_test[:,2],s=15, c = c, cmap=cmap)#, vmax = -0.5730, vmin = -2.63)\n",
    "#s3 = axs[2].scatter(z_test[:,1], z_test[:,2],s=15, c = c, cmap=cmap)\n",
    "\n",
    "axs[0].set_xlabel('Dim 1')\n",
    "axs[0].set_ylabel('Dim 2')\n",
    "axs[1].set_xlabel('Dim 1')\n",
    "axs[1].set_ylabel('Dim 3')\n",
    "#axs[2].set_xlabel('Dim 2')\n",
    "#axs[2].set_ylabel('Dim 3')\n",
    "\n",
    "cbar = fig.colorbar(s1, ax = axs[:], location = 'right')\n",
    "cbar.ax.set_ylabel('Relative enrichment')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_plot = []\n",
    "good_seqs = []\n",
    "thresh =  -1.7185 #4sigma\n",
    "for num, i in enumerate(RE):\n",
    "    if i<thresh:\n",
    "        pass\n",
    "    elif i>=thresh:\n",
    "        RE_plot.append(i)\n",
    "        good_seqs.append(num)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=[21,8])\n",
    "c = RE_plot\n",
    "cmap = plt.cm.coolwarm\n",
    "s1 = axs[0].scatter(z_test[good_seqs,0], z_test[good_seqs,1],s=10, c = c, cmap=cmap)\n",
    "s2 = axs[1].scatter(z_test[good_seqs,0], z_test[good_seqs,2],s=10, c = c, cmap=cmap)\n",
    "axs[0].set_xlabel('Dim 1')\n",
    "axs[0].set_ylabel('Dim 2')\n",
    "axs[1].set_xlabel('Dim 1')\n",
    "axs[1].set_ylabel('Dim 3')\n",
    "\n",
    "for i in range(2):\n",
    "    axs[i].set_xlim([-4,4])\n",
    "    axs[i].set_ylim([-4,4])\n",
    "\n",
    "cbar = fig.colorbar(s1, ax = axs[:], location = 'right')\n",
    "cbar.ax.set_ylabel('Relative enrichment')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_color = []\n",
    "for i in RE:\n",
    "    if i>thresh:\n",
    "        RE_color.append('r')\n",
    "    elif i<=thresh:\n",
    "        RE_color.append('b')\n",
    "    else:\n",
    "        RE_color.append('darkgrey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=[16,8])\n",
    "c = RE_color\n",
    "axs[0].scatter(z_test[:,0], z_test[:,1],s=15, c = c)\n",
    "axs[1].scatter(z_test[:,0], z_test[:,2],s=15, c = c)\n",
    "axs[0].set_xlabel('Dim 1')\n",
    "axs[0].set_ylabel('Dim 2')\n",
    "axs[1].set_xlabel('Dim 1')\n",
    "axs[1].set_ylabel('Dim 3')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sp = 150\n",
    "np.random.seed(98850) #700\n",
    "z_gen=np.random.normal(np.mean(z_test[good_seqs,:], axis = 0), \n",
    "                       np.var(z_test[good_seqs,:], axis = 0), (n_sp, d))\n",
    "data = torch.FloatTensor(z_gen).to(device)\n",
    "data = model.decode(data) # Use the decoding layer to generate new sequences.\n",
    "v_gen = data.cpu().detach().numpy()\n",
    "\n",
    "sample_list = []\n",
    "for i in range(n_sp): # number of sampling points\n",
    "        for k in range(10): # number of trials at each point\n",
    "            v_samp_nothot = toolkit.sample_seq(k, q, n, q_n, i, v_gen)\n",
    "            sample_list.append(v_samp_nothot)\n",
    "localsmp = toolkit.convert_alphabet(np.array(sample_list), parameters['index'], q_n)\n",
    "\n",
    "# Filtering to maximize diversity.\n",
    "chooseset = np.unique(localsmp) # Firstly remove duplicated sequences\n",
    "choose = [chooseset[0]]\n",
    "# Make min Hamming distance > 3 for every pair of sequences.\n",
    "for i in chooseset:\n",
    "    aa2inti = [toolkit.aa2int(j,toolkit.plm_dict.dict_aa2int) for j in choose]\n",
    "    if toolkit.minHamming(toolkit.aa2int(i, toolkit.plm_dict.dict_aa2int), aa2inti)>3:\n",
    "        choose.append(i)\n",
    "        \n",
    "# manually adjust a few sequences to reduct affect of misplacement in alignments.\n",
    "for num, i in enumerate(choose):\n",
    "    if i[33]=='-' and i[35] in ['R','K','N']:\n",
    "        choose[num] = i[:33]+i[34]+'G'+i[35:]\n",
    "    if 'EG-W' in i:\n",
    "        choose[num] = i.replace('EG-W', 'EGRW')\n",
    "    if 'SG-W' in i:\n",
    "        choose[num] = i.replace('SG-W', 'SGRW')\n",
    "    if 'KG-W' in i:\n",
    "        choose[num] = i.replace('KG-W', 'KGRW')\n",
    "    if i[0] == '-':\n",
    "        choose[num] = i.replace(i[0], 'N')\n",
    "        \n",
    "r1=0\n",
    "k1=0\n",
    "choose_ortho = []\n",
    "for i in choose:\n",
    "    if 'RWW' in i and i[11] in 'FYWRS' and i[18] in 'EGA':\n",
    "        choose_ortho.append(i)\n",
    "        r1+=1\n",
    "    if 'KWW' in i and i[11] in 'FYWRS' and i[18] in 'EGA':\n",
    "        choose_ortho.append(i)\n",
    "        k1+=1\n",
    "print('RWW ', r1)\n",
    "print('KWW ', k1)\n",
    "print('%d orthologs out of %d chosen sequences.' %(r1+k1, len(choose)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_local = [toolkit.aa2int(i,toolkit.plm_dict.dict_aa2int) for i in choose]\n",
    "int_msa = [toolkit.aa2int(i,toolkit.plm_dict.dict_aa2int) for i in parameters['seq']]\n",
    "min_Ham_local = []\n",
    "for i in int_local:\n",
    "    min_Ham_local.append(toolkit.minHamming(i, int_msa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### External validation: Analyze VAE generated sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is described in the beginning, before executing the following blocks, the python script *Generate_many_seqs* should be run to generate millions sequences:  \n",
    "  \n",
    "./Generate_many_seqs.py -g 20000 -r 1000 -n SH3  \n",
    "\n",
    "The evaluation is composed of 3 sections:\n",
    "* Predict mutation effect of Sho1  \n",
    "* Visualize the three optimizing objectives and compute Pareto shells to pick up sequences for experiments\n",
    "* Evaluate $10^4$ generated sequences by SCA (requires additional job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Predict mutation effect of Sho1**  \n",
    "Seems not good. Probably not good if including paralogs.  \n",
    "But may still contain useful information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change one position to alanine and test the average mutation effect through all sequences\n",
    "ind = parameters['index']\n",
    "diff_list = np.zeros([N,n])\n",
    "mut_alllist = []\n",
    "for num1, pos in enumerate(sho1):\n",
    "    for aa in ind[num1]:\n",
    "        mut_alllist.append(sho1[:num1] + aa + sho1[num1 + 1:])\n",
    "mut_potts, _ = toolkit.convert_potts(mut_alllist, ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mut,_,_ = model(torch.FloatTensor(mut_potts))\n",
    "mut_p = pred_mut.cpu().detach().numpy()\n",
    "p_diff = np.array(toolkit.make_logP(mut_potts, mut_p,q_n)) - log_p_list[sho1_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dms = -np.ones([n, 21])\n",
    "dict_dms = {'-ACDEFGHIKLMNPQRSTVWY'[i]:i for i in range(21)}\n",
    "plus = 0\n",
    "for num, pos in enumerate(sho1):\n",
    "    k = 0\n",
    "    for aa in ind[num]:\n",
    "        dms[num, dict_dms[aa]] = p_diff[plus + k]\n",
    "        k += 1\n",
    "    plus += len(ind[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [12,6])\n",
    "plt.imshow(dms.T, cmap = plt.cm.bwr, vmin = -.3, vmax = .3)\n",
    "plt.xticks(range(n), list('NFIYKAKALYPYDADDAYEISFEQNEILQVSDIEGRWWKARRNGETGIIPSNYVQLIDG'))\n",
    "plt.yticks(range(21), list('-ACDEFGHIKLMNPQRSTVWY'))\n",
    "plt.ylim([-.5,20.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**3. Evaluate $10^4$ generated sequences by SCA (optional)**  \n",
    "All technical details about SCA can be found in [this paper](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004817). Here we only demonstrate the codes to retrieve the distribution of correlations and sector decomposition, to show that our model have learned statistical patterns in the MSA.  \n",
    "\n",
    "If you want to analyze your VAE generated sequences by SCA, run the following command line to generate 10000 sequences without filtering. Notice that we need all of the sequences to maintain the statistic pattern, and 10000 sequences are enough for stable pattern.  \n",
    "  \n",
    "  ./Generate_many_seqs.py -n SH3 -c 1e4 -a\n",
    "  \n",
    "<font color='red'> \n",
    "* **Warning:**  \n",
    "    The SCA database file is very large (>700MB).  \n",
    "    Because SCA is expensive, do not enable SCA (-a) while generating large number (millions) of sequences. **\n",
    "</font> \n",
    "  \n",
    "After computing, we load the SCA databases for both MSA and VAE generated sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pickle.load(open(path + proteinname+'1e4_sca.db','rb'))\n",
    "Dseq = db['sequence']  #the results of scaProcessMSA\n",
    "Dsca = db['sca']       #the results of scaCore\n",
    "Dsect = db['sector']   #the results of scaSectorID\n",
    "\n",
    "db_ref = pickle.load(open('Inputs/sh3_59.db','rb'))\n",
    "Dseq_ref = db_ref['sequence']  \n",
    "Dsca_ref = db_ref['sca']       \n",
    "Dsect_ref = db_ref['sector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After processing, the alignment size of VAE generated sequences is %i sequences and %i positions\" % \\\n",
    "      (Dseq['Nseq'], Dseq['Npos']))\n",
    "print(\"With sequence weights, there are %i effective sequences\" % (Dseq['effseqs']))\n",
    "print()\n",
    "print(\"After processing, the alignment size of MSA is %i sequences and %i positions\" % \\\n",
    "      (Dseq_ref['Nseq'], Dseq_ref['Npos']))\n",
    "print(\"With sequence weights, there are %i effective sequences\" % (Dseq_ref['effseqs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaMat(alg, seqw=1, lbda=0, freq0=np.ones(20) / 21,):\n",
    "    N_seq, N_pos = alg.shape\n",
    "    N_aa = 20\n",
    "    if isinstance(seqw, int) and seqw == 1:\n",
    "        seqw = np.ones((1, N_seq))\n",
    "    freq1, freq2, freq0 = sca.freq(alg, Naa=N_aa, seqw=seqw, lbda=lbda, freq0=freq0)\n",
    "    return freq1, freq2 - np.outer(freq1, freq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_1e4 = np.load('Outputs/SH31e4gen_data.npz')\n",
    "int_new = [toolkit.aa2int(i,toolkit.plm_dict.dict_aa2int) for i in gen_1e4['seq'][:N]]\n",
    "new_f1, new_f2 = scaMat(np.array(int_new), seqw=1, lbda=0, freq0=np.ones(20) / 21,)\n",
    "int_msa = [toolkit.aa2int(i,toolkit.plm_dict.dict_aa2int) for i in parameters['seq']]\n",
    "msa_f1, msa_f2 = scaMat(np.array(int_msa), seqw=1, lbda=0, freq0=np.ones(20) / 21,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly show the $1^{st}$ ($D_i$) and $2^{nd}$ (${\\widetilde{C}_{ij}}$) order statistics of MSA between MSA and  sequences generated by VAE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "for i in range(2):\n",
    "    axs[i].set_ylabel('generated sequences')\n",
    "    axs[i].set_xlabel('MSA')\n",
    "    axs[i].grid(alpha = .5)\n",
    "\n",
    "axs[0].scatter(msa_f1,new_f1,s=5)\n",
    "axs[0].plot([0,1.],[0,1.],'r',lw=1)\n",
    "axs[0].set_title('1st order statistics')\n",
    "\n",
    "axs[1].scatter(msa_f2,new_f2,s=5)\n",
    "axs[1].plot([-.2,.25],[-.2,.25],'-r',lw=1)\n",
    "axs[1].set_title('2nd order statistics')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = sch.linkage(Dsca_ref['simMat'], method = 'complete', metric = 'cityblock')\n",
    "R = sch.dendrogram(Z, no_plot = True)\n",
    "ind = R['leaves']\n",
    "plt.imshow(Dsca_ref['simMat'][np.ix_(ind,ind)], vmin=0, vmax=1); plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sector decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sector_decompos(dsect,dseq, max_n_sec = 3):\n",
    "    Vpica = dsect['Vpica']\n",
    "    for k in range(dsect['kpos']):\n",
    "        iqr = scoreatpercentile(Vpica[:,k],75) - scoreatpercentile(Vpica[:,k],25)\n",
    "    sec_groups = [[i] for i in range(dsect['kpos'])]\n",
    "    sectors = list()\n",
    "\n",
    "    c = [0.4,0,0.7,0.15,0.9,0.5] \n",
    "    for n,k in enumerate(sec_groups):\n",
    "        s = sca.Unit()\n",
    "        all_items = list()\n",
    "        all_Vp = list()\n",
    "        for i in k: \n",
    "            all_items = all_items+dsect['ics'][i].items\n",
    "            all_Vp = all_Vp+list(dsect['ics'][i].vect)\n",
    "        svals = np.argsort(all_Vp)    \n",
    "        s.items = [all_items[i] for i in svals]\n",
    "        s.col = c[n]\n",
    "        sectors.append(s)\n",
    "        if n > max_n_sec-2:\n",
    "            break\n",
    "\n",
    "    ic_list = []\n",
    "    for i,k in enumerate(sectors):\n",
    "        sort_ipos = sorted(k.items)\n",
    "        ats_ipos = ([dseq['ats'][s] for s in sort_ipos])\n",
    "        ic_pymol = (', '.join(ats_ipos))\n",
    "        #ic_list.append([int(i) for i in ats_ipos])\n",
    "        ic_list.append(sort_ipos)\n",
    "        print('Sector %i is composed of %i positions:' % (i+1,len(ats_ipos)))\n",
    "        print(ic_pymol + \"\\n\")\n",
    "        if i>max_n_sec-2:\n",
    "            break\n",
    "    return ic_list\n",
    "\n",
    "def color_match(sg, sr): # Return a sg to match color with sr. sg: generated; sr: reference\n",
    "    sg_new = []\n",
    "    for i in sr:\n",
    "        sg_tmp = np.argmax([len(set(i) & set(sg_i)) for sg_i in sg])\n",
    "        sg_new.append(sg[sg_tmp])\n",
    "    return(sg_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate comparison, here are the SH3 sector definitions in a format suitable for pasting into pyMol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nMSA:')\n",
    "sr = Sector_decompos(Dsect_ref,Dseq_ref)\n",
    "print('Generated sequences:')\n",
    "sg = Sector_decompos(Dsect,Dseq)\n",
    "sg = color_match(sg,sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we illustrate the sector decomposition in the bar plot of the $D_i$ list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.array([i+1 for i in range(len(Dsca['Di']))])\n",
    "xvals_ref = np.array([i+1 for i in range(len(Dsca_ref['Di']))])\n",
    "xticks = np.arange(0,len(Dseq_ref['ats']),5)\n",
    "labels = [Dseq_ref['ats'][k] for k in xticks]\n",
    "\n",
    "cmap = plt.cm.tab20\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=cmap(np.linspace(0, 1, 10)))\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "fig, axs = plt.subplots(2,1, figsize=(12,6))\n",
    "axs[0].bar(xvals,Dsca['Di'], color='k')\n",
    "axs[1].bar(xvals_ref,Dsca_ref['Di'], color='k')\n",
    "axs[0].set_title('Generated sequences')\n",
    "axs[1].set_title('Natural sequences')\n",
    "\n",
    "for i in range(Dsect['kpos']):\n",
    "    axs[0].bar(xvals[sg[i]], Dsca['Di'][sg[i]], label = 'sector '+str(i+1))\n",
    "for i in range(Dsect_ref['kpos']):\n",
    "    axs[1].bar(xvals[sr[i]], Dsca_ref['Di'][sr[i]], label = 'sector '+str(i+1))\n",
    "\n",
    "for i in range(2):\n",
    "    axs[i].set_ylabel('Di')\n",
    "    axs[i].set_xticklabels(labels)\n",
    "    axs[i].set_xticks(xticks)\n",
    "    axs[i].grid()\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.xlabel('Amino acid position')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of pairwise Hamming distance of sequences in MSA and generated ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_seq, _ = toolkit.convert_potts(gen_1e4['seq'], parameters['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise = (n - new_seq @ new_seq.T).reshape([-1,1])\n",
    "pairwise_MSA = (n - v_traj_onehot @ v_traj_onehot.T).reshape([-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pairwise_MSA, int(max(pairwise_MSA)), density = True, alpha = 0.6, label='MSA',color='r')\n",
    "plt.hist(pairwise, int(max(pairwise)), density = True, alpha = 0.6, label='Generated',color='g')\n",
    "plt.xlabel('Pairwise Hamming distance')\n",
    "plt.ylabel('prob. den.')\n",
    "plt.title('SH3')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale('log')\n",
    "plt.hist(pairwise_MSA, int(max(pairwise_MSA)), density = True, alpha = 0.6, label='MSA',color='r')\n",
    "plt.hist(pairwise, int(max(pairwise)), density = True, alpha = 0.6, label='Generated',color='g')\n",
    "plt.xlabel('Pairwise Hamming distance')\n",
    "plt.ylabel('prob. den.')\n",
    "plt.title('SH3')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipca = IncrementalPCA(n_components=2, batch_size=3)\n",
    "ipca.fit(v_traj_onehot)\n",
    "msa_array = ipca.transform(v_traj_onehot) #PCA\n",
    "new_array = ipca.transform(new_seq) \n",
    "print(ipca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldim = 1\n",
    "\n",
    "plt.figure(figsize=[9,8]) # MSA\n",
    "plt.scatter(msa_array[:,0], msa_array[:,1],s=5, c=z_test[:, ldim],cmap=plt.cm.coolwarm) \n",
    "plt.colorbar()\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('color: latent dimension '+str(ldim+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "plt.figure(figsize=[8,8])\n",
    "plt.scatter(msa_array[:,0], msa_array[:,1],label = 'MSA n = '+str(N),s=5, c = 'cornflowerblue')\n",
    "#plt.scatter(msa_array[tst,0], msa_array[tst,1],label='Sho1 orthologs',s=5, c = 'r')\n",
    "plt.scatter(new_array[:,0], new_array[:,1],label='VAE generated seqs (n = 10000)'\n",
    "            ,s=5, alpha = 1, facecolors='none', edgecolors='k')\n",
    "#plt.scatter(new_ortho[:,0], new_ortho[:,1],s=5, c = 'g')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_traj_newall, _ = toolkit.convert_potts(gen_1e4['seq'], parameters['index'])\n",
    "z_new_all = model.get_z(torch.FloatTensor(v_traj_newall)).cpu().detach().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of all generated sequenced encoded in latent space\n",
    "fig, axs = plt.subplots(1,3,figsize=[32,10])\n",
    "axs[0].scatter(z_new_all[:,0], z_new_all[:,1],s=15, c=gen_1e4['logP'],cmap=plt.cm.coolwarm)\n",
    "axs[1].scatter(z_new_all[:,0], z_new_all[:,2],s=15, c=gen_1e4['logP'],cmap=plt.cm.coolwarm)\n",
    "axs[2].scatter(z_new_all[:,1], z_new_all[:,2],s=15, c=gen_1e4['logP'],cmap=plt.cm.coolwarm)\n",
    "\n",
    "axs[0].set_xlabel('Dim 1')\n",
    "axs[0].set_ylabel('Dim 2')\n",
    "axs[1].set_xlabel('Dim 1')\n",
    "axs[1].set_ylabel('Dim 3')\n",
    "axs[2].set_xlabel('Dim 2')\n",
    "axs[2].set_ylabel('Dim 3')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
